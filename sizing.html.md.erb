---
breadcrumb: PCF Metrics Documentation
title: Sizing PCF Metrics For Your System 
owner: PCF Metrics
---

<style>
    .note.warning {
        background-color: #fdd;
        border-color: #fbb
    }

    .note.warning:before {
        color: #f99;
     }
</style>

This topic describes how to configure Pivotal Cloud Foundry (PCF) Metrics for high availability. Operators can use these procedures to optimize PCF Metrics for high capacity.

For more information about PCF Metrics components, see the [PCF Metrics Product Architecture](./architecture.html) topic.

##<a id='metrics-datastore'></a> Configuring the Metrics Datastore

PCF Metrics uses three In-memory Database System (IMDS) components: 

* The Container Metrics IMDS stores container metrics data. 
* The Network Metrics IMDS stores network metrics data. 
* The App Event IMDS stores app event data. 

To customize PCF Metrics for high capacity, you can add memory to the VMs that run these components.

###<a id='metrics-considerations'></a> Considerations for Scaling

To calculate the correct memory allocation for each IMDS component of PCF Metrics, use the following formulas:

* **Container Metrics IMDS**: Each app instance produces around 500 kilobytes of metrics over a 24-hour period. To initially size this IMDS, multiply your number of app instances by 600 kilobytes. This should be your minimal setting for RAM on this IMDS. Consider over-allocating RAM to this resource to account for growth in app instances.     
* **Network Metrics IMDS**: Network traffic can be difficult to assess in advance. Consider allocating 4 megabytes of RAM per app as a starting point and then evaluating the memory use on this IMDS.      
* **App Event IMDS**: This resource depends on how frequently app events occur within an environment. Start by allocating 600 kilobytes per app instances and then multiply the number of expected app events over a 24-hour period by 20 bytes. As a rule of thumb, you can size this resource at 1 megabytes RAM per app instance, and then evaluate IMDS performance.      

Use these results as guidelines. Consider configuring your IMDS components with additional memory if your deployment adds additional app instances. 

###<a id='metrics-procedures'></a> Procedures for Scaling

<p class="note warning">
<strong>WARNING: </strong> If you modify the memory allocation of IMDS components, they will lose any data currently being stored.</p>

After determining the amount of memory required for each IMDS component, perform the following steps:

1. Navigate to the Ops Manager Installation Dashboard and click the **Metrics** tile.
1. From the **Settings** tab of the **Metrics** tile, click **Resource Config**.
1. Perform one or more of the following procedures, depending on which IMDS component you want to add memory to:
	<%= image_tag('imds.png') %>
	* To increase the memory limit for your App Event IMDS, locate the **App Event Data Store** job and select the dropdown menu under **VM Type** to select a VM with the desired amount of memory.
	* To increase the memory limit for your Network Metrics IMDS, locate the **Network Metrics Data Store** job and select the dropdown menu under **VM Type** to select a VM with the desired amount of memory. 
	* To increase the memory limit for your Container Metrics IMDS, locate the **Container Metrics Data Store** job and select the dropdown menu under **VM Type** to select a VM with the desired amount of memory.

##<a id='log-datastore'></a> Configuring the Log Datastore

PCF Metrics uses the [Elasticsearch](https://www.elastic.co/) search engine to store logs. Each Elasticsearch node contains multiple shards of log data, divided by time slice. To customize PCF Metrics for high capacity, you can scale the number of Elasticsearch nodes.

###<a id='log-considerations'></a> Considerations for Scaling

To determine the number of Elasticsearch nodes required for PCF Metrics, consider how many logs the apps in your deployment emit and the average size of each log.

If your average log size is 1 kilobyte, and each node has 1 terabyte of available disk space, then each node has a maximum storage capacity of 1 billion log messages. If your apps emit 3 billion logs over a 24-hour period, you need at least 3 nodes to hold the data and 3 additional nodes for high-availability replication. 

This example assumes that your apps emit logs at a continuous rate over 24 hours. However, apps typically do not emit logs continuously. If your apps emit 2 billion of the 3 billion logs between 8 AM and 4 PM, you must determine the minimum node-to-shard ratio to accommodate that rate over the 8-hour period. Because your apps emit 1 billion logs over a 4 hour span, you need at least 6 nodes (24 hours/6 nodes = 4 hours worth of shards per node) to hold the data and an additional 6 nodes for high-availability replication.   

###<a id='log-procedures'></a> Procedures for Scaling

<p class="note warning">
<strong>WARNING: </strong> If you modify the number of Elasticsearch instances, the Elasticsearch cluster will lose any data currently being stored.</p>

After determining the number of Elasticsearch nodes needed for your deployment, perform the following steps to scale your nodes:

1. Navigate to the Ops Manager Installation Dashboard and click the **Metrics** tile.
1. From the **Settings** tab of the **Metrics** tile, click **Resource Config**.
1. Locate the **ElasticSearchData** job and select the dropdown menu under **Instances** to change the number of instances.
	<br>
	<%= image_tag('elasticsearch.png') %>
1. Click **Save**.  

##<a id='ingestor'></a> Configuring the Ingestor

PCF Metrics deploys the Ingestor as an app within PCF. The Ingestor consumes logs and metrics from the Loggregator Firehose, sending metrics to Kafka and logs to the Logqueue app. To customize PCF Metrics for high capacity, you can scale the number of Ingestor app instances and increase the amount of memory per instance.

###<a id='ingestor-considerations'></a> Considerations for Scaling

Because apps emit logs at different volumes and frequencies, you should not scale the Ingestor by matching the number of Ingestor instances to the number of app instances in your deployment.

Because Ingestor performance is affected by Loggregator performance, it can be difficult to determine in advance the proper configuration. Because of the ease in scaling these components, we recommend starting with a minimal configuration then evaluating performance over a period of time and scaling 

The Ingestor app can handle relatively large loads. For high availability, you must have at least two instances of the Ingestor app running. If your deployment runs fewer than 2000 app instances, two instances of the Ingestor app are sufficient. 

###<a id='ingestor-procedures'></a> Procedures for Scaling

<p class="note warning"><strong>WARNING: </strong> If you decrease the number of Ingestor instances, you may lose data currently being processed on the instances you eliminate.</p>

After determining the number of Ingestor app instances needed for your deployment, perform the following steps to scale the Ingestor:

1. Target your Cloud Controller with the Cloud Foundry Command Line Interface (cf CLI). If you have not installed the cf CLI, see the [Installing the cf CLI](http://docs.pivotal.io/pivotalcf/1-7/cf-cli/install-go-cli.html) topic.
	<pre class="terminal">
	$ cf api api.YOUR-SYSTEM-DOMAIN
	Setting api endpoint to api.YOUR-SYSTEM-DOMAIN...
	OK
	API endpoint:   <span>https:</span>//api.YOUR-SYSTEM-DOMAIN (API version: 2.54.0)
	Not logged in. Use 'cf login' to log in.
	</pre>

1. Log in with your UAA administrator credentials. To retrieve these credentials, navigate to the **Pivotal Elastic Runtime** tile in the Ops Manager Installation Dashboard and click **Credentials**. Under **UAA**, click **Link to Credential** next to **Admin Credentials** and record the password.
	<pre class="terminal">
	$ cf login
	API endpoint: <span>https:</span>//api.YOUR-SYSTEM-DOMAIN

	Email> admin
	Password>
	Authenticating...
	OK

1. When prompted, target the `metrics` space.
	<pre class="terminal">
	Targeted org system

	Select a space (or press enter to skip):
	<span>1</span>. system
	<span>2</span>. notifications-with-ui
	<span>3</span>. autoscaling
	<span>4</span>. metrics

	Space> 4
	Targeted space metrics

	API endpoint:   <span>https:</span>//api.YOUR-SYSTEM-DOMAIN (API version: 2.54.0)
	User:           admin
	Org:            system
	Space:          metrics
	</pre>

1. Scale your Ingestor app to the desired number of instances:
	<pre class="terminal">$ cf scale metrics-ingestor -i INSTANCE-NUMBER</pre>
1. Evaluate the CPU and memory load on your Ingestor instances:
	<pre class="terminal">
	$ cf app metrics-ingestor
	Showing health and status for app metrics-ingestor in org system / space metrics as admin...
	OK
	<br>
	requested state: started
	instances: 1/1
	usage: 1G x 1 instances
	urls: 
	last uploaded: Sat Apr 23 16:11:29 UTC 2016
	stack: cflinuxfs2
	buildpack: binary_buildpack 
	<br>	
	     state     since                    cpu    memory        disk           details
	<span>#</span>0   running   2016-07-21 03:49:58 PM   2.9%   13.5M of 1G   12.9M of 1G
	</pre>

	If your average memory usage exceeds 50% or your CPU consistently averages over 85%, add more instances with `cf scale metrics-ingestor -i INSTANCE-NUMBER`.
	<br><br>
	In general, you should scale the Ingestor app by adding additional instances. However, you can also scale the Ingestor app by increasing the amount of memory per instance:

	<pre class="terminal">$ cf scale metrics-ingestor -m NEW-MEMORY-LIMIT</pre>

	For more information about scaling app instances, see the [Scaling an Application Using cf scale](http://docs.pivotal.io/pivotalcf/1-7/devguide/deploy-apps/cf-scale.html) topic.

##<a id='logqueue'></a> Configuring the Logqueue

PCF Metrics deploys the Logqueue as an app within PCF. The Logqueue consumes logs from the Ingestor and forwards them to the Elasticsearch nodes. To customize PCF Metrics for high capacity, you can scale the number of Logqueue app instances and increase the amount of memory per instance.

###<a id='logqueue-considerations'></a> Considerations for Scaling

To determine the number of Logqueue instances required for your deployment, multiply your number of Elasticsearch nodes by 1.5. This is a general estimate and you may need fewer instances depending on your deployment. To optimize resource allocation, provision fewer instances initially and increase instances until you achieve desired performance.

###<a id='logqueue-procedures'></a> Procedures for Scaling

To modify your Logqueue app instances, you must first target your Cloud Controller, log in with your UAA administrator credentials, and target the `metrics` space by following steps 1-3 in the [previous section](#ingestor-procedures).

To scale your Logqueue app instances, perform the following command:
<pre class="terminal">$ cf scale metrics-logqueue -i INSTANCE-NUMBER</pre>

To scale the memory limit per Logqueue app instance, perform the following command:
<pre class="terminal">$ cf scale metrics-logqueue -m NEW-MEMORY-LIMIT</pre>

<p class="note warning">
<strong>WARNING: </strong> If you decrease the number of Logqueue instances, you may lose data currently being processed on the instances you eliminate.</p>

##<a id='troubleshooting'></a> Troubleshooting

To troubleshoot PCF Metrics, review the list of common issues below. To resolve certain issues, you must check the health of your Elasticsearch cluster.

###<a id='common-issues'></a> Common Issues

The following list includes common issues and their solutions:

* **WebSocket Disconnects**: If you see Websocket disconnects logs in the Ingestor app, consider adding additional Ingestor instances. The Firehose may be dropping the Ingestor connection to avoid back pressure.
* **"503" Error**: If you encounter "503" errors when accessing the PCF Metrics UI in your browser, your Elasticsearch nodes may have become unresponsive. Check the Elasticsearch index health by following the procedure below, and consider adding additional Elasticsearch nodes.

###<a id='check-cluster-health'></a> Check Elasticsearch Cluster Health 

To check the health of your Elasticsearch cluster, perform the following steps:

1. Retrieve the IP address of your Elasticsearch master node by navigating to the **Metrics** tile in the Ops Manager Installation Dashboard, clicking the **Status** tab, and recording the IP address next to **ElasticSearchMaster**.
	<%= image_tag('elasticsearch-ip.png') %>
1. SSH into the Ops Manager VM by following the instructions in [SSH into Ops Manager](http://docs.pivotal.io/pivotalcf/1-7/customizing/trouble-advanced.html#ssh).
1. From the Ops Manager VM, use `curl` to target the IP address of your Elasticsearch master node. Follow the instructions in the [Cluster Health](https://www.elastic.co/guide/en/elasticsearch/reference/2.2/_cluster_health.html) topic of the Elasticsearch documentation.


